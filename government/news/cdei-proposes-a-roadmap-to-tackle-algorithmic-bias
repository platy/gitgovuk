<div class="govuk-grid-row">
  <div class="govuk-grid-column-two-thirds">
    <div class="gem-c-title govuk-!-margin-top-8 govuk-!-margin-bottom-8">
    <span class="govuk-caption-xl gem-c-title__context">
      Press release
    </span>
  <h1 class="gem-c-title__text gem-c-title__text--long">
    CDEI proposes a roadmap to tackle algorithmic bias
  </h1>
</div>
  </div>
  
  <div class="govuk-grid-column-two-thirds">
    
  <p class="gem-c-lead-paragraph">New report from the Centre for Data Ethics and Innovation proposes significant measures to tackle the risks of algorithmic bias.</p>

  </div>
</div>

<div class="govuk-grid-row">
  <div class="metadata-logo-wrapper">
    <div class="govuk-grid-column-two-thirds metadata-column">
        <div class="app-c-publisher-metadata" lang="en">
    <div class="app-c-published-dates " lang="en">
    Published 27 November 2020
</div>

      <div class="app-c-publisher-metadata__other">
        <dl data-module="track-click">
            <dt class="app-c-publisher-metadata__term">
              From:
            </dt>
            <dd class="app-c-publisher-metadata__definition" data-module="gem-toggle">
                <span class="app-c-publisher-metadata__definition-sentence">
                  <a class="govuk-link" href="/government/organisations/centre-for-data-ethics-and-innovation">Centre for Data Ethics and Innovation</a>
                </span>
            </dd>
        </dl>
      </div>
  </div>

    </div>
    <div class="govuk-grid-column-one-third">
    </div>
  </div>
</div>




<div class="govuk-grid-row">
  <div class="govuk-grid-column-two-thirds content-bottom-margin">
    <div class="responsive-bottom-margin">
      <figure class="app-c-figure" lang="en">
  <img class="app-c-figure__image" src="https://assets.publishing.service.gov.uk/government/uploads/system/uploads/default_news_organisation_image_data/file/469/s300_notebook.jpeg" alt></img>
    <figcaption class="app-c-figure__figcaption">

    </figcaption>
</figure>

      
<div class="gem-c-govspeak govuk-govspeak direction-ltr" data-module="govspeak">
    
        <div class="govspeak">
<p>A major review from the Centre for Data Ethics and Innovation (CDEI), which draws on a detailed analysis of the use of algorithms in four sectors (financial services, local government, policing and recruitment), proposes significant measures for government, regulators and industry to act on to tackle the risks of algorithmic bias.</p>

<p>Key recommendations include:</p>

<ul>
  <li>Government should place a mandatory transparency obligation on all public sector organisations using algorithms that have an impact on significant decisions affecting individuals.</li>
</ul>

<p>*Organisations should be actively using data to identify and mitigate bias. They should make sure that they understand the capabilities and limitations of algorithmic tools, and carefully consider how they will ensure fair treatment of individuals.</p>

<ul>
  <li>Government should issue guidance that clarifies the application of the Equality Act to algorithmic decision-making. This should include guidance on the collection of data to measure bias, as well as the lawfulness of bias mitigation techniques (some of which risk introducing positive discrimination, which is illegal under the Equality Act).</li>
</ul>

<p>The CDEI, the UK government’s advisory body on the responsible use of AI and data-driven technology, has proposed a roadmap that government, regulators and industry can take to increase fairness and reduce bias, while also ensuring that the UK regulatory ecosystem is set up to support responsible innovation.</p>

<p>The measures are designed to produce a step change in the behaviour of all organisations making life-changing decisions on the basis of data, with a focus on improving accountability and transparency. The report emphasises that organisations are responsible for their decisions - whether they have been made by an algorithm or a team of humans - and offers guidance for organisational leaders and boards to enhance accountability.</p>

<p>The CDEI recommends that the government should place a transparency obligation on all public sector organisations using algorithms which support significant decisions. This would include information about how algorithms are used in the overall decision-making process, and steps taken to ensure fair treatment of individuals.</p>

<p>The CDEI argues that there is an opportunity here: effective use of data can enable organisations to shine a light on practices that may otherwise go unseen, and identify the drivers of bias. The report recommends that government and regulators provide clear guidance about how organisations can actively use data to tackle current and historic bias. This guidance should address the misconception that data protection law prevents the collection or usage of data for monitoring or addressing discrimination.</p>

<p>The review was informed by public engagement to gain a deeper understanding of attitudes towards algorithmic decision-making, drawing on methodologies including survey and behavioural science research. A large scale survey, conducted with Deltapoll, found that the majority of respondents were aware of the use of algorithms to support decision-making (around 6 out of 10). Of those respondents who were aware of the use of algorithms, respondents were most aware of their use in financial services (more than 5 in 10), in contrast to local government (around 3 in 10). The results suggest that the public are more concerned that the outcome of decision-making is fair, rather than whether algorithms are used to inform these judgements. There is public support for data - including age (net agreement of +59%), ethnicity (+59%) and sex (+39%) - to be used for tackling algorithmic bias in recruitment.</p>

<p>The review points to the need for an ecosystem of industry standards and professional services to help organisations address algorithmic bias in the UK and beyond. This presents an opportunity for the UK: leadership in this area can not only ensure fairness for British citizens, but can also unlock growth by incubating new industries in responsible technology.</p>

<p>To catalyse this, the CDEI has initiated a programme of work on AI assurance, in which it will identify what is needed to develop a strong AI accountability ecosystem in the UK. Other related CDEI work includes: working with the Government Digital Service (GDS) to pilot an approach to algorithmic transparency; supporting a police force and a local authority to apply lessons learnt and develop practical governance structures; and active public engagement to build understanding of the values that citizens want reflected in new models of data governance.</p>

<p>In its report, the CDEI calls for national leadership and coordination to drive the pace of change, and urges the government to be clear on where responsibilities sit for tracking progress.</p>

<p><strong>Adrian Weller, Board Member for the Centre for Data Ethics and Innovation, said:</strong></p>

<p>“It is vital that we work hard now to get this right as adoption of algorithmic decision-making increases. Government, regulators and industry need to work together with interdisciplinary experts, stakeholders and the public to ensure that algorithms are used to promote fairness, not undermine it. The Centre for Data Ethics and Innovation has today set out a range of measures to help the UK to achieve this, with a focus on enhancing transparency and accountability in decision-making processes that have a significant impact on individuals. Not only does the report propose a roadmap to tackle the risks, but it highlights the opportunity that good use of data presents to address historical unfairness and avoid new biases in key areas of life.”</p>

<p><strong>Simon McDougall, Deputy Commissioner - Regulatory Innovation and Technology for the Information Commissioner’s Office, said:</strong></p>

<p>“We welcome and support the findings of the report and want to echo the Centre for Data Ethics and Innovation’s concerns around algorithmic bias. When developed and used responsibly, algorithms can transform society for the better. But there is also significant risk that algorithms can exacerbate issues of fairness and inequality. This often impacts the most vulnerable or marginalised people. Data protection law requires fair and transparent uses of data in algorithms, gives people rights in relation to automated decision-making, and demands that the outcome from the use of algorithms does not result in unfair or discriminatory impacts. The ICO has prioritised the data protection implications of AI for a long time and has produced guidance for organisations on the use of AI.”</p>
</div>

</div>
    </div>

    <div class="dont-print responsive-bottom-margin">
        <div class="gem-c-share-links " data-module="track-click">
      <h2 class="gem-c-share-links__title">Share this page</h2>

    <ul class="gem-c-share-links__list">
        <li class="gem-c-share-links__list-item">
          <a target="_blank" rel="noopener noreferrer external" data-track-category="social media" data-track-action="facebook" data-track-options="{"socialAction":"share","socialNetwork":"facebook","socialTarget":"https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.gov.uk%2Fgovernment%2Fnews%2Fcdei-proposes-a-roadmap-to-tackle-algorithmic-bias"}" class="gem-c-share-links__link " href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.gov.uk%2Fgovernment%2Fnews%2Fcdei-proposes-a-roadmap-to-tackle-algorithmic-bias">
            <span class="gem-c-share-links__link-icon">
                <svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" aria-hidden="true">
                  <path fill="currentColor" d="M31.006 0H.993A.997.997 0 0 0 0 .993v30.014c0 .55.452.993.993.993h30.013a.998.998 0 0 0 .994-.993V.993A.999.999 0 0 0 31.006 0z"></path>
                  <path fill="#FFF" d="M17.892 10.751h1.787V8.009L17.216 8c-2.73 0-3.352 2.045-3.352 3.353v1.828h-1.581v2.824h1.581V24h3.322v-7.995h2.242l.291-2.824h-2.533V11.52c.001-.623.415-.769.706-.769z"></path>
                </svg>

            </span>          <span class="govuk-visually-hidden">
              Share on
          </span>
          Facebook
</a>
        </li>
        <li class="gem-c-share-links__list-item">
          <a target="_blank" rel="noopener noreferrer external" data-track-category="social media" data-track-action="twitter" data-track-options="{"socialAction":"share","socialNetwork":"twitter","socialTarget":"https://twitter.com/share?url=https%3A%2F%2Fwww.gov.uk%2Fgovernment%2Fnews%2Fcdei-proposes-a-roadmap-to-tackle-algorithmic-bias\u0026text=CDEI%20proposes%20a%20roadmap%20to%20tackle%20algorithmic%20bias"}" class="gem-c-share-links__link " href="https://twitter.com/share?url=https%3A%2F%2Fwww.gov.uk%2Fgovernment%2Fnews%2Fcdei-proposes-a-roadmap-to-tackle-algorithmic-bias&text=CDEI%20proposes%20a%20roadmap%20to%20tackle%20algorithmic%20bias">
            <span class="gem-c-share-links__link-icon">
                <svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" aria-hidden="true">
                  <path fill="currentColor" d="M31.007 0H.993A.999.999 0 0 0 0 .993v30.014c0 .55.452.993.993.993h30.014a.997.997 0 0 0 .993-.993V.993A.998.998 0 0 0 31.007 0z"></path>
                  <path fill="#FFF" d="M8 21.027a9.286 9.286 0 0 0 5.032 1.475c6.038 0 9.34-5.002 9.34-9.339 0-.143-.004-.284-.012-.425a6.619 6.619 0 0 0 1.639-1.699c-.6.265-1.234.439-1.885.516a3.287 3.287 0 0 0 1.443-1.816 6.571 6.571 0 0 1-2.086.797 3.28 3.28 0 0 0-5.592 2.993 9.311 9.311 0 0 1-6.766-3.43 3.294 3.294 0 0 0-.443 1.651 3.28 3.28 0 0 0 1.46 2.732 3.278 3.278 0 0 1-1.488-.411v.041a3.288 3.288 0 0 0 2.633 3.22 3.28 3.28 0 0 1-1.481.055 3.285 3.285 0 0 0 3.065 2.281 6.59 6.59 0 0 1-4.076 1.404A6.76 6.76 0 0 1 8 21.027z"></path>
                </svg>

            </span>          <span class="govuk-visually-hidden">
              Share on
          </span>
          Twitter
</a>
        </li>
    </ul>
</div>
    </div>

    <div class="app-c-published-dates " lang="en">
    Published 27 November 2020
</div>

  </div>

  
<div class="govuk-grid-column-one-third">
  
<div class="gem-c-contextual-sidebar">
    

  <a class="gem-c-transition-countdown gem-c-transition-countdown--cta govuk-link" data-module="track-click" data-track-category="relatedLinkClicked" data-track-action="1.0 Transition" data-track-label="/transition" data-track-dimension="Check you’re ready for 2021" data-track-dimension-index="29" lang="en" dir="ltr" href="/transition">
      <h2 class="gem-c-transition-countdown__title">Brexit transition</h2>
    <p class="gem-c-transition-countdown__countdown">
      <span class="gem-c-transition-countdown__countdown-number">3</span><span class="gem-c-transition-countdown__countdown-number">5</span> <span class="gem-c-transition-countdown__countdown-text">days to go</span>
</p>  <p class="gem-c-transition-countdown__text">Check you’re ready for 2021</p>

</a>


    



</div>

</div>

</div>


  <div class="govuk-grid-row">
    <div class="govuk-grid-column-two-thirds">
        
  <div class="gem-c-contextual-footer">
    
  <div class="gem-c-related-navigation">





      <nav role="navigation" class="gem-c-related-navigation__nav-section" aria-labelledby="related-nav-topics-3421fea5" data-module="gem-toggle">

    <h2 id="related-nav-topics-3421fea5" class="gem-c-related-navigation__sub-heading gem-c-related-navigation__sub-heading--footer" data-track-count="footerRelatedItemSection">Explore the topic</h2>

  <ul class="gem-c-related-navigation__link-list" data-module="track-click">


        <li class="gem-c-related-navigation__link"><a class="gem-c-related-navigation__section-link gem-c-related-navigation__section-link--footer" data-track-category="relatedLinkClicked" data-track-action="1.1 Explore the topic" data-track-label="/government/all" data-track-options="{"dimension28":"1","dimension29":"Government"}" href="/government/all">Government</a></li>

  </ul>
</nav>

  </div>

  </div>


    </div>
  </div>